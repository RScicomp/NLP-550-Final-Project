{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import re\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def tokenize(q1, q2):\n",
    "    \"\"\"\n",
    "        q1 and q2 are sentences/questions. Function returns a list of tokens for both.\n",
    "    \"\"\"\n",
    "    return word_tokenize(q1), word_tokenize(q2)\n",
    "\n",
    "\n",
    "def posTag(q1, q2):\n",
    "    \"\"\"\n",
    "        q1 and q2 are lists. Function returns a list of POS tagged tokens for both.\n",
    "    \"\"\"\n",
    "    return nltk.pos_tag(q1), nltk.pos_tag(q2)\n",
    "\n",
    "\n",
    "def stemmer(tag_q1, tag_q2):\n",
    "    \"\"\"\n",
    "        tag_q = tagged lists. Function returns a stemmed list.\n",
    "    \"\"\"\n",
    "\n",
    "    stem_q1 = []\n",
    "    stem_q2 = []\n",
    "\n",
    "    for token in tag_q1:\n",
    "        stem_q1.append(stem(token))\n",
    "\n",
    "    for token in tag_q2:\n",
    "        stem_q2.append(stem(token))\n",
    "\n",
    "    return stem_q1, stem_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lesk(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.meanings = {}\n",
    "        for word in sentence:\n",
    "            self.meanings[word] = ''\n",
    "\n",
    "    def getSenses(self, word):\n",
    "        # print word\n",
    "        return wn.synsets(word.lower())\n",
    "\n",
    "    def getGloss(self, senses):\n",
    "\n",
    "        gloss = {}\n",
    "\n",
    "        for sense in senses:\n",
    "            gloss[sense.name()] = []\n",
    "\n",
    "        for sense in senses:\n",
    "            gloss[sense.name()] += word_tokenize(sense.definition())\n",
    "\n",
    "        return gloss\n",
    "    def getAll(self, word):\n",
    "        senses = self.getSenses(word)\n",
    "\n",
    "        if senses == []:\n",
    "            return {word.lower(): senses}\n",
    "\n",
    "        return self.getGloss(senses)\n",
    "\n",
    "    def Score(self, set1, set2):\n",
    "        # Base\n",
    "        overlap = 0\n",
    "\n",
    "        # Step\n",
    "        for word in set1:\n",
    "            if word in set2:\n",
    "                overlap += 1\n",
    "\n",
    "        return overlap\n",
    "\n",
    "    def overlapScore(self, word1, word2):\n",
    "\n",
    "        gloss_set1 = self.getAll(word1)\n",
    "        if self.meanings[word2] == '':\n",
    "            gloss_set2 = self.getAll(word2)\n",
    "        else:\n",
    "            # print 'here'\n",
    "            gloss_set2 = self.getGloss([wn.synset(self.meanings[word2])])\n",
    "\n",
    "        # print gloss_set2\n",
    "        score = {}\n",
    "        for i in gloss_set1.keys():\n",
    "            score[i] = 0\n",
    "            for j in gloss_set2.keys():\n",
    "                score[i] += self.Score(gloss_set1[i], gloss_set2[j])\n",
    "\n",
    "        bestSense = None\n",
    "        max_score = 0\n",
    "        for i in gloss_set1.keys():\n",
    "            if score[i] > max_score:\n",
    "                max_score = score[i]\n",
    "                bestSense = i\n",
    "\n",
    "        return bestSense, max_score\n",
    "\n",
    "    def lesk(self, word, sentence):\n",
    "        maxOverlap = 0\n",
    "        context = sentence\n",
    "        word_sense = []\n",
    "        meaning = {}\n",
    "\n",
    "        senses = self.getSenses(word)\n",
    "\n",
    "        for sense in senses:\n",
    "            meaning[sense.name()] = 0\n",
    "\n",
    "        for word_context in context:\n",
    "            if not word == word_context:\n",
    "                score = self.overlapScore(word, word_context)\n",
    "                if score[0] == None:\n",
    "                    continue\n",
    "                meaning[score[0]] += score[1]\n",
    "\n",
    "        if senses == []:\n",
    "            return word, None, None\n",
    "        self.meanings[word] = max(meaning.keys(), key=lambda x: meaning[x])\n",
    "\n",
    "        return word, self.meanings[word], wn.synset(self.meanings[word]).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def path(set1, set2):\n",
    "    return wn.path_similarity(set1, set2)\n",
    "\n",
    "\n",
    "def wup(set1, set2):\n",
    "    return wn.wup_similarity(set1, set2)\n",
    "\n",
    "\n",
    "def edit(word1, word2):\n",
    "    if float(edit_distance(word1, word2)) == 0.0:\n",
    "        return 0.0\n",
    "    return 1.0 / float(edit_distance(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePath(q1, q2):\n",
    "\n",
    "    R = np.zeros((len(q1), len(q2)))\n",
    "\n",
    "    for i in range(len(q1)):\n",
    "        for j in range(len(q2)):\n",
    "            if q1[i][1] == None or q2[j][1] == None:\n",
    "                sim = edit(q1[i][0], q2[j][0])\n",
    "            else:\n",
    "                sim = path(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
    "\n",
    "            if sim == None:\n",
    "                sim = edit(q1[i][0], q2[j][0])\n",
    "\n",
    "            R[i, j] = sim\n",
    "\n",
    "    # print R\n",
    "\n",
    "    return R\n",
    "def computeWup(q1, q2):\n",
    "\n",
    "    R = np.zeros((len(q1), len(q2)))\n",
    "\n",
    "    for i in range(len(q1)):\n",
    "        for j in range(len(q2)):\n",
    "            if q1[i][1] == None or q2[j][1] == None:\n",
    "                sim = edit(q1[i][0], q2[j][0])\n",
    "            else:\n",
    "                sim = wup(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
    "\n",
    "            if sim == None:\n",
    "                sim = edit(q1[i][0], q2[j][0])\n",
    "\n",
    "            R[i, j] = sim\n",
    "\n",
    "    # print R\n",
    "\n",
    "    return R\n",
    "\n",
    "def overallSim(q1, q2, R):\n",
    "\n",
    "    sum_X = 0.0\n",
    "    sum_Y = 0.0\n",
    "\n",
    "    for i in range(len(q1)):\n",
    "        max_i = 0.0\n",
    "        for j in range(len(q2)):\n",
    "            if R[i, j] > max_i:\n",
    "                max_i = R[i, j]\n",
    "        sum_X += max_i\n",
    "\n",
    "    for i in range(len(q1)):\n",
    "        max_j = 0.0\n",
    "        for j in range(len(q2)):\n",
    "            if R[i, j] > max_j:\n",
    "                max_j = R[i, j]\n",
    "        sum_Y += max_j\n",
    "        \n",
    "    if (float(len(q1)) + float(len(q2))) == 0.0:\n",
    "        return 0.0\n",
    "        \n",
    "    overall = (sum_X + sum_Y) / (2 * (float(len(q1)) + float(len(q2))))\n",
    "\n",
    "    return overall\n",
    "\n",
    "\n",
    "    tokens_q1, tokens_q2 = tokenize(q1, q2)\n",
    "    # stem_q1, stem_q2 = stemmer(tokens_q1, tokens_q2)\n",
    "    tag_q1, tag_q2 = posTag(tokens_q1, tokens_q2)\n",
    "\n",
    "    sentence = []\n",
    "    for i, word in enumerate(tag_q1):\n",
    "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
    "            sentence.append(word[0])\n",
    "\n",
    "    sense1 = Lesk(sentence)\n",
    "    sentence1Means = []\n",
    "    for word in sentence:\n",
    "        sentence1Means.append(sense1.lesk(word, sentence))\n",
    "\n",
    "    sentence = []\n",
    "    for i, word in enumerate(tag_q2):\n",
    "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
    "            sentence.append(word[0])\n",
    "\n",
    "    sense2 = Lesk(sentence)\n",
    "    sentence2Means = []\n",
    "    for word in sentence:\n",
    "        sentence2Means.append(sense2.lesk(word, sentence))\n",
    "    # for i, word in enumerate(sentence1Means):\n",
    "    #     print sentence1Means[i][0], sentence2Means[i][0]\n",
    "\n",
    "    R1 = computePath(sentence1Means, sentence2Means)\n",
    "    R2 = computeWup(sentence1Means, sentence2Means)\n",
    "\n",
    "    R = (R1 + R2) / 2\n",
    "\n",
    "    # print R\n",
    "\n",
    "    return overallSim(sentence1Means, sentence2Means, R)\n",
    "def semanticSimilarity(q1, q2):\n",
    "\n",
    "    tokens_q1, tokens_q2 = tokenize(q1, q2)\n",
    "    # stem_q1, stem_q2 = stemmer(tokens_q1, tokens_q2)\n",
    "    tag_q1, tag_q2 = posTag(tokens_q1, tokens_q2)\n",
    "\n",
    "    sentence = []\n",
    "    for i, word in enumerate(tag_q1):\n",
    "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
    "            sentence.append(word[0])\n",
    "\n",
    "    sense1 = Lesk(sentence)\n",
    "    sentence1Means = []\n",
    "    for word in sentence:\n",
    "        sentence1Means.append(sense1.lesk(word, sentence))\n",
    "\n",
    "    sentence = []\n",
    "    for i, word in enumerate(tag_q2):\n",
    "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
    "            sentence.append(word[0])\n",
    "\n",
    "    sense2 = Lesk(sentence)\n",
    "    sentence2Means = []\n",
    "    for word in sentence:\n",
    "        sentence2Means.append(sense2.lesk(word, sentence))\n",
    "    # for i, word in enumerate(sentence1Means):\n",
    "    #     print sentence1Means[i][0], sentence2Means[i][0]\n",
    "\n",
    "    R1 = computePath(sentence1Means, sentence2Means)\n",
    "    R2 = computeWup(sentence1Means, sentence2Means)\n",
    "\n",
    "    R = (R1 + R2) / 2\n",
    "\n",
    "    # print R\n",
    "    return overallSim(sentence1Means, sentence2Means, R)\n",
    "import nltk\n",
    "STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "\n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)\n",
    "\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.135515873015873"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semanticSimilarity(\"Hello what is your name?\", \"My name is Richard\")\n",
    "#https://www.kaggle.com/antriksh5235/semantic-similarity-using-wordnet\n",
    "\n",
    "#cosine similarity\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
